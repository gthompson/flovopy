{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d08fc811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef make_asl_heatmap_for_ampmap(\\n    startdate: Union[str, \"UTCDateTime\", pd.Timestamp, datetime],\\n    enddate:   Union[str, \"UTCDateTime\", pd.Timestamp, datetime],\\n    *,\\n    localprojectdir: Path,\\n    lat_col: str = \"lat\",\\n    lon_col: str = \"lon\",\\n    amp_col: str = \"DR\",\\n    # Optional row filters (only applied if column exists)\\n    misfit_max: Optional[float] = None,\\n    nsta_min: Optional[int] = None,\\n    connectedness_min: Optional[float] = None,\\n    azgap_max: Optional[float] = None,\\n    dr_min: Optional[float] = None,\\n    dr_max: Optional[float] = None,\\n    # Plot options\\n    inventory=None,\\n    cmap: str = \"turbo\",\\n    log_scale: bool = True,\\n    node_spacing_m: int = 50,\\n    region: Optional[List[float]] = None,\\n    dem_tif: Optional[Union[str, Path]] = None,\\n    title_fmt: str = \"ASL Heatmap (AMPMAP): {start}–{end} UTC\",\\n    outfile: Optional[Union[str, Path]] = None,  # e.g. \"heatmaps/ampmap_{start}_{end}.png\"\\n    # File discovery\\n    glob_pattern: str = \"*.csv\",\\n    recursive: bool = False,\\n    topo_kw: Optional[Dict[str, Any]] = None,\\n    return_df: bool = True,\\n) -> Tuple[\"pygmt.Figure\", Optional[pd.DataFrame]]:\\n    \"\"\"\\n    Load AMPMAP CSVs from a directory, filter by time and optional quality fields,\\n    concatenate rows, and plot a heatmap.\\n    \"\"\"\\n    lp = Path(localprojectdir)\\n    if not lp.exists():\\n        raise FileNotFoundError(f\"localprojectdir not found: {lp}\")\\n\\n    start_dt = _to_utc_datetime(startdate)\\n    end_dt   = _to_utc_datetime(enddate)\\n    if end_dt < start_dt:\\n        raise ValueError(\"enddate is earlier than startdate\")\\n\\n    rows: List[pd.DataFrame] = []\\n    csv_iter = lp.rglob(glob_pattern) if recursive else lp.glob(glob_pattern)\\n\\n    for csv_path in csv_iter:\\n        try:\\n            df = pd.read_csv(csv_path)\\n        except Exception as e:\\n            print(f\"[warn] Could not read {csv_path}: {e}\")\\n            continue\\n\\n        # Require t, lat, lon, DR (or custom amp_col)\\n        needed = {\"t\", lat_col, lon_col, amp_col}\\n        if not needed.issubset(df.columns):\\n            continue\\n\\n        t_parsed = pd.to_datetime(df[\"t\"], utc=True, errors=\"coerce\")\\n        mask_time = (t_parsed >= pd.Timestamp(start_dt)) & (t_parsed <= pd.Timestamp(end_dt))\\n        df = df.loc[mask_time].copy()\\n        if df.empty:\\n            continue\\n\\n        # Optional filters (only if columns exist)\\n        if misfit_max is not None and \"misfit\" in df.columns:\\n            df = df[df[\"misfit\"] <= misfit_max]\\n        if nsta_min is not None and \"nsta\" in df.columns:\\n            df = df[df[\"nsta\"] >= nsta_min]\\n        if connectedness_min is not None and \"connectedness\" in df.columns:\\n            df = df[df[\"connectedness\"] >= connectedness_min]\\n        if azgap_max is not None and \"azgap\" in df.columns:\\n            df = df[df[\"azgap\"] <= azgap_max]\\n        if dr_min is not None:\\n            df = df[df[amp_col] >= dr_min]\\n        if dr_max is not None:\\n            df = df[df[amp_col] <= dr_max]\\n        if df.empty:\\n            continue\\n\\n        df = df[[lat_col, lon_col, amp_col]].astype(float)\\n        finite = np.isfinite(df[lat_col]) & np.isfinite(df[lon_col]) & np.isfinite(df[amp_col])\\n        df = df.loc[finite]\\n        if not df.empty:\\n            rows.append(df)\\n\\n    if not rows:\\n        raise ValueError(\"No AMPMAP CSV rows matched the date range and filters in the specified directory.\")\\n\\n    df_all = pd.concat(rows, ignore_index=True)\\n\\n    start_str = pd.Timestamp(start_dt).strftime(\"%Y-%m-%d %H:%M:%S\")\\n    end_str   = pd.Timestamp(end_dt).strftime(\"%Y-%m-%d %H:%M:%S\")\\n    title = title_fmt.format(start=start_str, end=end_str)\\n\\n    # Allow tokens in outfile\\n    if outfile:\\n        start_safe = pd.Timestamp(start_dt).strftime(\"%Y%m%dT%H%M%S\")\\n        end_safe   = pd.Timestamp(end_dt).strftime(\"%Y%m%dT%H%M%S\")\\n        outfile = str(outfile).format(start=start_safe, end=end_safe)\\n\\n    fig = plot_heatmap_colored(\\n        df_all,\\n        lat_col=lat_col,\\n        lon_col=lon_col,\\n        amp_col=amp_col,\\n        inventory=inventory,\\n        cmap=cmap,\\n        log_scale=log_scale,\\n        node_spacing_m=node_spacing_m,\\n        outfile=outfile,\\n        region=region,\\n        title=title,\\n        dem_tif=dem_tif,\\n        topo_kw=topo_kw,\\n    )\\n\\n    return (fig, df_all) if return_df else (fig, None)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Tuple, Union, Dict, Any\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# Optional: allow ObsPy types without a hard dependency\n",
    "try:\n",
    "    from obspy import UTCDateTime  # type: ignore\n",
    "except Exception:  # pragma: no cover\n",
    "    UTCDateTime = object  # for typing only\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# .mat → DataFrame\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# Matches 'YYYY-mm-dd-HHMM-SS' (optionally with a third digit in SS)\n",
    "_START_RE = re.compile(r\"(?P<stamp>\\d{4}-\\d{2}-\\d{2}-\\d{4}-\\d{2,3})\")\n",
    "\n",
    "# Optional arrays we’ll copy into the DataFrame if present\n",
    "_OPTIONAL_SERIES_KEYS: Tuple[str, ...] = (\n",
    "    \"rms_fin\", \"res_ry_fin\", \"res_ss_fin\", \"res_by_fin\",\n",
    "    \"res_gh_fin\", \"res_gb_fin\", \"res_wh_fin\", \"res_lg_fin\", \"res_mh_fin\",\n",
    "    \"start_point\", \"end_point\", \"FFT_v\", \"cut_off_v\", \"damp_v\",\n",
    ")\n",
    "\n",
    "\n",
    "def _parse_start_time_from_filename(path: Union[str, Path]) -> datetime:\n",
    "    \"\"\"\n",
    "    Extract UTC datetime from 'YYYY-mm-dd-HHMM-SS*.mat' found in filename.\n",
    "    If seconds has 3 digits, trim to 2 (e.g., 42[5] → 42).\n",
    "    \"\"\"\n",
    "    name = Path(path).name\n",
    "    m = _START_RE.search(name)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Cannot infer start time from filename: {name}\")\n",
    "    stamp = m.group(\"stamp\")\n",
    "    parts = stamp.split(\"-\")\n",
    "    if len(parts[-1]) == 3:  # trim SSS → SS\n",
    "        stamp = \"-\".join(parts[:-1] + [parts[-1][:2]])\n",
    "    dt = datetime.strptime(stamp, \"%Y-%m-%d-%H%M-%S\")\n",
    "    return dt.replace(tzinfo=timezone.utc)\n",
    "\n",
    "\n",
    "def _as_scalar(x: Any, default: Optional[float] = None) -> Optional[float]:\n",
    "    \"\"\"Return x as a Python float scalar if possible; otherwise default.\"\"\"\n",
    "    if x is None:\n",
    "        return default\n",
    "    arr = np.asarray(x)\n",
    "    if arr.size == 0:\n",
    "        return default\n",
    "    return float(arr.squeeze())\n",
    "\n",
    "\n",
    "def mat_to_dataframe(\n",
    "    mat_path: Union[str, Path],\n",
    "    *,\n",
    "    fs_hz: float = 75.0,\n",
    "    save_csv: bool = True,\n",
    "    outdir: Optional[Union[str, Path]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert one AMPMAP .mat file into a tidy DataFrame with columns:\n",
    "      t (ISO8601 'Z'), lat, lon, DR, and optional Q + residuals if present.\n",
    "\n",
    "    Time vector is inferred from filename start time and (time_step_v / fs_hz).\n",
    "    \"\"\"\n",
    "    mat_path = Path(mat_path)\n",
    "    d: Dict[str, Any] = loadmat(mat_path, squeeze_me=True)\n",
    "\n",
    "    loc_fin = np.asarray(d.get(\"loc_fin\"))\n",
    "    amp_fin = np.asarray(d.get(\"amp_fin\"))\n",
    "    if loc_fin is None or amp_fin is None:\n",
    "        raise ValueError(f\"{mat_path.name}: requires 'loc_fin' and 'amp_fin'\")\n",
    "    if loc_fin.ndim != 2 or loc_fin.shape[1] != 2:\n",
    "        raise ValueError(f\"{mat_path.name}: loc_fin must be (N, 2) of [lat, lon]\")\n",
    "\n",
    "    # Frame count\n",
    "    N = min(loc_fin.shape[0], int(np.asarray(amp_fin).size))\n",
    "    loc_fin = loc_fin[:N, :]\n",
    "    amp_fin = np.asarray(amp_fin).reshape(-1)[:N]\n",
    "\n",
    "    # Scalars and timing\n",
    "    Q = _as_scalar(d.get(\"Q_v\"))\n",
    "    time_step_samples = _as_scalar(d.get(\"time_step_v\"), default=256.0) or 256.0\n",
    "    dt_sec = float(time_step_samples) / float(fs_hz)\n",
    "\n",
    "    t0 = _parse_start_time_from_filename(mat_path)\n",
    "    times = [\n",
    "        (t0 + timedelta(seconds=i * dt_sec)).isoformat().replace(\"+00:00\", \"Z\") for i in range(N)\n",
    "    ]\n",
    "\n",
    "    # Base DataFrame\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"t\": times,\n",
    "            \"lat\": loc_fin[:, 0],\n",
    "            \"lon\": loc_fin[:, 1],\n",
    "            \"DR\": amp_fin.astype(float),\n",
    "        }\n",
    "    )\n",
    "    if Q is not None:\n",
    "        df[\"Q\"] = Q\n",
    "\n",
    "    # Optional arrays (scalar repeated or vector per frame)\n",
    "    for key in _OPTIONAL_SERIES_KEYS:\n",
    "        val = d.get(key)\n",
    "        if val is None:\n",
    "            continue\n",
    "        arr = np.asarray(val)\n",
    "        if arr.ndim == 0 or arr.size == 1:\n",
    "            df[key] = _as_scalar(arr)\n",
    "        else:\n",
    "            flat = arr.squeeze()\n",
    "            if flat.ndim == 1 and flat.size >= N:\n",
    "                df[key] = flat[:N].astype(float)\n",
    "\n",
    "    if save_csv:\n",
    "        odir = Path(outdir) if outdir is not None else mat_path.parent\n",
    "        odir.mkdir(parents=True, exist_ok=True)\n",
    "        out_csv = odir / f\"{mat_path.stem}.csv\"\n",
    "        df.to_csv(out_csv, index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def batch_convert_ampmap_dir(\n",
    "    mat_dir: Union[str, Path],\n",
    "    *,\n",
    "    fs_hz: float = 75.0,\n",
    "    outdir: Optional[Union[str, Path]] = None,\n",
    "    glob_pattern: str = \"*.mat\",\n",
    "    tag: str = \"VSAM_mean_13s_surface_v1.5_Q25_F8_2d_l2\",\n",
    "    overwrite: bool = False,\n",
    ") -> List[Path]:\n",
    "    \"\"\"\n",
    "    Convert all matching .mat files to CSV and store them in a per-event/tag layout:\n",
    "\n",
    "        <outdir or mat_dir>/<mat_stem>/<tag>/source_<tag>.csv\n",
    "\n",
    "    Returns a list of written CSV Paths.\n",
    "    \"\"\"\n",
    "    mat_dir = Path(mat_dir)\n",
    "    base_out = Path(outdir) if outdir is not None else mat_dir\n",
    "\n",
    "    written: List[Path] = []\n",
    "    for mat_path in sorted(mat_dir.glob(glob_pattern)):\n",
    "        # Convert in-memory; write to the desired structured path\n",
    "        df = mat_to_dataframe(mat_path, fs_hz=fs_hz, save_csv=False)\n",
    "        csv_path = base_out / mat_path.stem / tag / f\"source_{tag}.csv\"\n",
    "        csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if csv_path.exists() and not overwrite:\n",
    "            # Skip if exists and overwrite=False\n",
    "            continue\n",
    "\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        written.append(csv_path)\n",
    "\n",
    "    return written\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Time parsing helper (shared across your code)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def _to_utc_datetime(dt_like: Union[str, datetime, \"UTCDateTime\", pd.Timestamp]) -> datetime:\n",
    "    \"\"\"Normalize input into a timezone-aware UTC datetime.\"\"\"\n",
    "    if isinstance(dt_like, pd.Timestamp):\n",
    "        return (\n",
    "            dt_like.tz_convert(\"UTC\").to_pydatetime()\n",
    "            if dt_like.tzinfo\n",
    "            else dt_like.tz_localize(\"UTC\").to_pydatetime()\n",
    "        )\n",
    "    if UTCDateTime and isinstance(dt_like, UTCDateTime):  # type: ignore\n",
    "        return dt_like.datetime.replace(tzinfo=timezone.utc)\n",
    "    if isinstance(dt_like, datetime):\n",
    "        return dt_like.astimezone(timezone.utc) if dt_like.tzinfo else dt_like.replace(tzinfo=timezone.utc)\n",
    "    if isinstance(dt_like, str):\n",
    "        try:\n",
    "            d = datetime.fromisoformat(dt_like)\n",
    "        except ValueError:\n",
    "            d = datetime.strptime(dt_like, \"%Y-%m-%d-%H%M-%S\")\n",
    "        return d.astimezone(timezone.utc) if d.tzinfo else d.replace(tzinfo=timezone.utc)\n",
    "    raise TypeError(f\"Unsupported datetime-like type: {type(dt_like)}\")\n",
    "\n",
    "'''\n",
    "def make_asl_heatmap_for_ampmap(\n",
    "    startdate: Union[str, \"UTCDateTime\", pd.Timestamp, datetime],\n",
    "    enddate:   Union[str, \"UTCDateTime\", pd.Timestamp, datetime],\n",
    "    *,\n",
    "    localprojectdir: Path,\n",
    "    lat_col: str = \"lat\",\n",
    "    lon_col: str = \"lon\",\n",
    "    amp_col: str = \"DR\",\n",
    "    # Optional row filters (only applied if column exists)\n",
    "    misfit_max: Optional[float] = None,\n",
    "    nsta_min: Optional[int] = None,\n",
    "    connectedness_min: Optional[float] = None,\n",
    "    azgap_max: Optional[float] = None,\n",
    "    dr_min: Optional[float] = None,\n",
    "    dr_max: Optional[float] = None,\n",
    "    # Plot options\n",
    "    inventory=None,\n",
    "    cmap: str = \"turbo\",\n",
    "    log_scale: bool = True,\n",
    "    node_spacing_m: int = 50,\n",
    "    region: Optional[List[float]] = None,\n",
    "    dem_tif: Optional[Union[str, Path]] = None,\n",
    "    title_fmt: str = \"ASL Heatmap (AMPMAP): {start}–{end} UTC\",\n",
    "    outfile: Optional[Union[str, Path]] = None,  # e.g. \"heatmaps/ampmap_{start}_{end}.png\"\n",
    "    # File discovery\n",
    "    glob_pattern: str = \"*.csv\",\n",
    "    recursive: bool = False,\n",
    "    topo_kw: Optional[Dict[str, Any]] = None,\n",
    "    return_df: bool = True,\n",
    ") -> Tuple[\"pygmt.Figure\", Optional[pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Load AMPMAP CSVs from a directory, filter by time and optional quality fields,\n",
    "    concatenate rows, and plot a heatmap.\n",
    "    \"\"\"\n",
    "    lp = Path(localprojectdir)\n",
    "    if not lp.exists():\n",
    "        raise FileNotFoundError(f\"localprojectdir not found: {lp}\")\n",
    "\n",
    "    start_dt = _to_utc_datetime(startdate)\n",
    "    end_dt   = _to_utc_datetime(enddate)\n",
    "    if end_dt < start_dt:\n",
    "        raise ValueError(\"enddate is earlier than startdate\")\n",
    "\n",
    "    rows: List[pd.DataFrame] = []\n",
    "    csv_iter = lp.rglob(glob_pattern) if recursive else lp.glob(glob_pattern)\n",
    "\n",
    "    for csv_path in csv_iter:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] Could not read {csv_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Require t, lat, lon, DR (or custom amp_col)\n",
    "        needed = {\"t\", lat_col, lon_col, amp_col}\n",
    "        if not needed.issubset(df.columns):\n",
    "            continue\n",
    "\n",
    "        t_parsed = pd.to_datetime(df[\"t\"], utc=True, errors=\"coerce\")\n",
    "        mask_time = (t_parsed >= pd.Timestamp(start_dt)) & (t_parsed <= pd.Timestamp(end_dt))\n",
    "        df = df.loc[mask_time].copy()\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        # Optional filters (only if columns exist)\n",
    "        if misfit_max is not None and \"misfit\" in df.columns:\n",
    "            df = df[df[\"misfit\"] <= misfit_max]\n",
    "        if nsta_min is not None and \"nsta\" in df.columns:\n",
    "            df = df[df[\"nsta\"] >= nsta_min]\n",
    "        if connectedness_min is not None and \"connectedness\" in df.columns:\n",
    "            df = df[df[\"connectedness\"] >= connectedness_min]\n",
    "        if azgap_max is not None and \"azgap\" in df.columns:\n",
    "            df = df[df[\"azgap\"] <= azgap_max]\n",
    "        if dr_min is not None:\n",
    "            df = df[df[amp_col] >= dr_min]\n",
    "        if dr_max is not None:\n",
    "            df = df[df[amp_col] <= dr_max]\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        df = df[[lat_col, lon_col, amp_col]].astype(float)\n",
    "        finite = np.isfinite(df[lat_col]) & np.isfinite(df[lon_col]) & np.isfinite(df[amp_col])\n",
    "        df = df.loc[finite]\n",
    "        if not df.empty:\n",
    "            rows.append(df)\n",
    "\n",
    "    if not rows:\n",
    "        raise ValueError(\"No AMPMAP CSV rows matched the date range and filters in the specified directory.\")\n",
    "\n",
    "    df_all = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "    start_str = pd.Timestamp(start_dt).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    end_str   = pd.Timestamp(end_dt).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    title = title_fmt.format(start=start_str, end=end_str)\n",
    "\n",
    "    # Allow tokens in outfile\n",
    "    if outfile:\n",
    "        start_safe = pd.Timestamp(start_dt).strftime(\"%Y%m%dT%H%M%S\")\n",
    "        end_safe   = pd.Timestamp(end_dt).strftime(\"%Y%m%dT%H%M%S\")\n",
    "        outfile = str(outfile).format(start=start_safe, end=end_safe)\n",
    "\n",
    "    fig = plot_heatmap_colored(\n",
    "        df_all,\n",
    "        lat_col=lat_col,\n",
    "        lon_col=lon_col,\n",
    "        amp_col=amp_col,\n",
    "        inventory=inventory,\n",
    "        cmap=cmap,\n",
    "        log_scale=log_scale,\n",
    "        node_spacing_m=node_spacing_m,\n",
    "        outfile=outfile,\n",
    "        region=region,\n",
    "        title=title,\n",
    "        dem_tif=dem_tif,\n",
    "        topo_kw=topo_kw,\n",
    "    )\n",
    "\n",
    "    return (fig, df_all) if return_df else (fig, None)\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ed2c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from obspy import UTCDateTime\n",
    "\n",
    "HOME        = Path.home()\n",
    "PROJECTDIR  = HOME / \"Dropbox\" / \"BRIEFCASE\" / \"SSADenver\"\n",
    "AMP_CSV_DIR = PROJECTDIR / \"AMPMAP_CSV\"\n",
    "DEM_DEFAULT = PROJECTDIR / \"metadata\" / \"MONTSERRAT_DEM_WGS84_MASTER.tif\"\n",
    "HEATMAP_DIR = PROJECTDIR / \"heatmaps\"\n",
    "HEATMAP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define time windows\n",
    "start = UTCDateTime(2001, 2, 6, 12, 0, 0)\n",
    "mid   = UTCDateTime(2001, 2, 16, 0, 0, 0)\n",
    "end   = UTCDateTime(2001, 7, 2, 12, 0, 0)\n",
    "windows = [(start, mid), (mid, end)]\n",
    "\n",
    "# Common args\n",
    "common = dict(\n",
    "    localprojectdir=AMP_CSV_DIR,\n",
    "    cmap=\"turbo\",\n",
    "    log_scale=True,\n",
    "    node_spacing_m=100,\n",
    "    dem_tif=DEM_DEFAULT,\n",
    "    return_df=True,\n",
    "    region=[-62.208, -62.138, 16.681, 16.751],\n",
    "    outfile_pattern=str(HEATMAP_DIR / \"ampmap_{tag}_{start}_{end}.png\"),\n",
    ")\n",
    "\n",
    "results = {}\n",
    "for s, e in windows:\n",
    "    res = make_asl_heatmap_from_events(\n",
    "        startdate=s,\n",
    "        enddate=e,\n",
    "        **common,\n",
    "    )\n",
    "    results[(s, e)] = res\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f773afe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap from new ASL locations but with ampmap settings\n",
    "from flovopy.asl.wrappers import make_asl_heatmaps_per_tag\n",
    "tag = \"VSAM_mean_5s_surface_v1.5_Q23_F8_2d_l2\"\n",
    "startt = UTCDateTime(\"2001-02-06\")\n",
    "endt = UTCDateTime(\"2001-03-04\")\n",
    "res = make_asl_heatmaps_per_tag(\n",
    "    startt, \n",
    "    endt,\n",
    "    localprojectdir=\"/Users/thompsong/Dropbox/AMPMAP_RESULTS\",\n",
    "    tag=[tag],\n",
    "    nsta_min=5, \n",
    "    misfit_max=0.30,\n",
    "    outfile_pattern = str(PROJECTDIR / \"heatmaps\" /f\"{tag}_{startt}_{endt}.png\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c38d5196",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = Path.home()\n",
    "PROJECTDIR      = HOME / \"Dropbox\" / \"BRIEFCASE\" / \"SSADenver\"\n",
    "LOCALPROJECTDIR = HOME / \"work\" / \"PROJECTS\" / \"SSADenver_local\"\n",
    "from flovopy.asl.wrappers import make_asl_heatmap_from_events\n",
    "tag = \"VSAM_mean_5s_surface_v1.5_Q23_F8_2d_l2\"\n",
    "METADATA_DIR    = PROJECTDIR / \"metadata\" \n",
    "DEM_DEFAULT     = METADATA_DIR / \"MONTSERRAT_DEM_WGS84_MASTER.tif\"\n",
    "startt = UTCDateTime(\"2001-02-06\")\n",
    "endt = UTCDateTime(\"2001-03-04\")\n",
    "\n",
    "fig = make_asl_heatmap_from_events(\n",
    "    startdate=startt,\n",
    "    enddate=endt,\n",
    "    localprojectdir=\"/Users/thompsong/Dropbox/AMPMAP_RESULTS\",\n",
    "    tag=tag,\n",
    "    # region=[-62.25, -62.10, 16.65, 16.78],\n",
    "    dem_tif=DEM_DEFAULT,\n",
    "    # outfile=\"asl_heatmap_{start}_{end}.png\",\n",
    "    #nsta_min=5, \n",
    "    #misfit_max=0.30,\n",
    "    # return_df=True,\n",
    "    # verbose=True,\n",
    "    outfile = str(PROJECTDIR / \"heatmaps\" /f\"{tag}_{startt}_{endt}.png\"),\n",
    ")\n",
    "\n",
    "startt = UTCDateTime(\"2001-03-04\")\n",
    "endt = UTCDateTime(\"2001-03-09 12:00:00\")\n",
    "fig = make_asl_heatmap_from_events(\n",
    "    startdate=startt,\n",
    "    enddate=endt,\n",
    "    localprojectdir=\"/Users/thompsong/Dropbox/AMPMAP_RESULTS\",\n",
    "    tag=tag,\n",
    "    # region=[-62.25, -62.10, 16.65, 16.78],\n",
    "    dem_tif=DEM_DEFAULT,\n",
    "    # outfile=\"asl_heatmap_{start}_{end}.png\",\n",
    "    #nsta_min=5, \n",
    "    #misfit_max=0.30,\n",
    "    # return_df=True,\n",
    "    # verbose=True,\n",
    "    outfile = str(PROJECTDIR / \"heatmaps\" /f\"{tag}_{startt}_{endt}.png\"),\n",
    "\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flovopy_plus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
