{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6ce9277",
   "metadata": {},
   "source": [
    "# 1. Set up parameters for ASL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94679f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from obspy import read_inventory\n",
    "from importlib import reload\n",
    "from flovopy.asl.wrappers2 import run_single_event, find_event_files, run_all_events\n",
    "from flovopy.core.mvo import dome_location, REGION_DEFAULT\n",
    "from flovopy.processing.sam import VSAM, DSAM \n",
    "from flovopy.asl.config import ASLConfig\n",
    "# -------------------------- Config --------------------------\n",
    "# directories\n",
    "HOME = Path.home()\n",
    "PROJECTDIR      = HOME / \"Dropbox\" / \"BRIEFCASE\" / \"SSADenver\"\n",
    "LOCALPROJECTDIR = HOME / \"work\" / \"PROJECTS\" / \"SSADenver_local\"\n",
    "OUTPUT_DIR      = LOCALPROJECTDIR / \"asl_results\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "INPUT_DIR       = PROJECTDIR / \"ASL_inputs\" / \"biggest_pdc_events\"\n",
    "GLOBAL_CACHE    = PROJECTDIR / \"asl_global_cache\"\n",
    "METADATA_DIR    = PROJECTDIR / \"metadata\" \n",
    "STATION_CORRECTIONS_DIR = PROJECTDIR / \"station_correction_analysis\"\n",
    "\n",
    "# master files\n",
    "INVENTORY_XML   = METADATA_DIR / \"MV_Seismic_and_GPS_stations.xml\"\n",
    "DEM_DEFAULT     = METADATA_DIR / \"MONTSERRAT_DEM_WGS84_MASTER.tif\"\n",
    "GRIDFILE_DEFAULT= METADATA_DIR / \"MASTER_GRID_MONTSERRAT.pkl\"\n",
    "\n",
    "# parameters for envelopes and cross-correlation\n",
    "SMOOTH_SECONDS  = 1.0\n",
    "MAX_LAG_SECONDS = 8.0\n",
    "MIN_XCORR       = 0.5\n",
    "\n",
    "# other parameters\n",
    "DIST_MODE = \"3d\" # or 2d. will essentially squash Montserrat topography and stations onto a sea-level plane, ignored elevation data, e.g. for computing distances\n",
    "\n",
    "# Inventory of Montserrat stations\n",
    "from obspy import read_inventory\n",
    "INV     = read_inventory(INVENTORY_XML)\n",
    "print(f\"[INV] Networks: {len(INV)}  Stations: {sum(len(n) for n in INV)}  Channels: {sum(len(sta) for net in INV for sta in net)}\")\n",
    "\n",
    "# Montserrat station corrections estimated from regionals\n",
    "station_corrections_csv = STATION_CORRECTIONS_DIR / \"station_gains_intervals.csv\"\n",
    "annual_station_corrections_csv = STATION_CORRECTIONS_DIR / \"station_gains_intervals_by_year.csv\"\n",
    "station_corrections_df = pd.read_csv(station_corrections_csv)\n",
    "annual_station_corrections_df = pd.read_csv(annual_station_corrections_csv)\n",
    "\n",
    "# Montserrat pre-defined Grid (from 02 tutorial)\n",
    "from flovopy.asl.grid import Grid\n",
    "gridobj = Grid.load(GRIDFILE_DEFAULT)\n",
    "print(gridobj)\n",
    "\n",
    "\n",
    "# Montserrat constants\n",
    "from flovopy.core.mvo import dome_location, REGION_DEFAULT\n",
    "print(\"Dome (assumed source) =\", dome_location)\n",
    "\n",
    "# events and wrappers\n",
    "from flovopy.asl.wrappers2 import run_single_event, find_event_files, run_all_events\n",
    "event_files = list(find_event_files(INPUT_DIR))\n",
    "eventcsvfile = Path(OUTPUT_DIR) / \"mseed_files.csv\"\n",
    "if not eventcsvfile.is_file():\n",
    "    rows = [{\"num\": num, \"f\": str(f)} for num, f in enumerate(event_files)]\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(eventcsvfile, index=False)\n",
    "best_file_nums  = [35, 36, 40, 52, 82, 83, 84, 116, 310, 338]\n",
    "best_event_files = [event_files[i] for i in best_file_nums]\n",
    "print(f'Best miniseed files are: {best_event_files}')\n",
    "REFINE_SECTOR = False   # enable triangular dome-to-sea refinement\n",
    "\n",
    "# Parameters to pass for making pygmt topo maps\n",
    "topo_kw = {\n",
    "    \"inv\": INV,\n",
    "    \"add_labels\": True,\n",
    "    \"cmap\": \"gray\",\n",
    "    \"region\": REGION_DEFAULT,\n",
    "    \"dem_tif\": DEM_DEFAULT,  # basemap shading from your GeoTIFF - but does not actually seem to use this unless topo_color=True and cmap=None\n",
    "    \"frame\": True,\n",
    "    \"dome_location\": dome_location,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd2c7cd",
   "metadata": {},
   "source": [
    "# Run events from last cell, one event at a time, to check it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed3982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an ASL Configuration. This is inherited by various downstream functions\n",
    "# This describes the physical parameters, the station metadata, the grid, the misfit algorithm, etc.\n",
    "cfg = ASLConfig(\n",
    "    inventory=INV,\n",
    "    output_base=OUTPUT_DIR, # str?\n",
    "    gridobj=gridobj,\n",
    "    global_cache=GLOBAL_CACHE,\n",
    "    station_correction_dataframe=station_corrections_df,\n",
    "    wave_kind = \"body\", # \"surface\" or \"body\"\n",
    "    speed = 3.0, # km/s\n",
    "    Q = 100, # attenuation quality factor\n",
    "    peakf = 2.0, # Hz\n",
    "    dist_mode = DIST_MODE, # or \"2d\"\n",
    "    misfit_engine = \"r2\", # l2, r2, lin?\n",
    "    window_seconds = 5.0, # length of time window for amplitude measurement\n",
    "    min_stations = 5, # minimum number of stations required to locate event\n",
    "    sam_class = VSAM, # or DSAM\n",
    "    sam_metric = \"VT\", # or one of \"mean\", \"median\", \"max\", \"rms\", \"VLP\", or \"LP\"\n",
    "    debug=True,\n",
    ")\n",
    "cfg.build()\n",
    "summaries = []\n",
    "\n",
    "REFINE_SECTOR=False\n",
    "for i, ev in zip(best_file_nums, best_event_files):\n",
    "    print(f\"[{i}/{len(event_files)}] {ev}\")\n",
    "    result = run_single_event(\n",
    "        mseed_file=str(ev),\n",
    "        cfg=cfg,\n",
    "        refine_sector=REFINE_SECTOR,\n",
    "        station_gains_df=None,\n",
    "        switch_event_ctag = True,\n",
    "        topo_kw=topo_kw,\n",
    "        mseed_units='m/s', # default units for miniseed files being used - probably \"Counts\" or \"m/s\"        \n",
    "        reduce_time=True,\n",
    "        debug=True,\n",
    "    )\n",
    "    summaries.append(result)\n",
    "\n",
    "# Summarize\n",
    "df = pd.DataFrame(summaries)\n",
    "display(df)\n",
    "\n",
    "summary_csv = Path(OUTPUT_DIR) / f\"{cfg.tag()}__summary.csv\"\n",
    "df.to_csv(summary_csv, index=False)\n",
    "print(f\"Summary saved to: {summary_csv}\")\n",
    "\n",
    "if not df.empty:\n",
    "    n_ok = int((~df.get(\"error\").notna()).sum()) if \"error\" in df.columns else len(df)\n",
    "    print(f\"Success: {n_ok}/{len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d83281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import replace\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# --- build the baseline cfg first (as you already do) ---\n",
    "# cfg = ASLConfig(...).build()\n",
    "\n",
    "def cfg_variants_from(baseline_cfg: ASLConfig):\n",
    "    \"\"\"\n",
    "    Create ONE-change variants from a built baseline ASLConfig.\n",
    "    Returns dict[label] -> built ASLConfig.\n",
    "    \"\"\"\n",
    "    variants = {}\n",
    "\n",
    "    # 1) Q 100 -> 23\n",
    "    v_Q23 = replace(baseline_cfg, Q=23).build()\n",
    "    variants[\"Q23\"] = v_Q23\n",
    "\n",
    "    # 2) speed 3.0 -> 1.5 km/s\n",
    "    v_v15 = replace(baseline_cfg, speed=1.5).build()\n",
    "    variants[\"v1.5\"] = v_v15\n",
    "\n",
    "    # 3) window 5 -> 2 s\n",
    "    v_win2 = replace(baseline_cfg, window_seconds=2.0).build()\n",
    "    variants[\"win2s\"] = v_win2\n",
    "\n",
    "    # 4) metric VT -> mean\n",
    "    v_mean = replace(baseline_cfg, sam_metric=\"mean\").build()\n",
    "    variants[\"metric_mean\"] = v_mean\n",
    "\n",
    "    # 5) station corr ON -> OFF\n",
    "    v_nosc = replace(baseline_cfg, station_correction_dataframe=None).build()\n",
    "    variants[\"no_station_corr\"] = v_nosc\n",
    "\n",
    "    return variants\n",
    "\n",
    "\n",
    "# --- make variants ---\n",
    "variants = cfg_variants_from(baseline_cfg=cfg)   # dict[label] -> built ASLConfig\n",
    "\n",
    "def csv_for_run(mseed_file: str | Path, cfg: ASLConfig) -> Path:\n",
    "    \"\"\"Locate the source CSV written by wrappers2.run_single_event for a given cfg.\"\"\"\n",
    "    mseed_file = Path(mseed_file)\n",
    "    event_dir = Path(cfg.output_base) / mseed_file.stem\n",
    "    products_dir = event_dir / Path(cfg.outdir).name\n",
    "    refined = products_dir / f\"{cfg.tag()}_refined.csv\"\n",
    "    plain   = products_dir / f\"{cfg.tag()}.csv\"\n",
    "    return refined if refined.exists() else plain\n",
    "\n",
    "# Build baseline config\n",
    "cfg = ASLConfig(\n",
    "    inventory=INV,\n",
    "    output_base=OUTPUT_DIR,\n",
    "    gridobj=gridobj,\n",
    "    global_cache=GLOBAL_CACHE,\n",
    "    station_correction_dataframe=station_corrections_df,\n",
    "    wave_kind=\"body\",\n",
    "    speed=3.0,\n",
    "    Q=100,\n",
    "    peakf=2.0,\n",
    "    dist_mode=DIST_MODE,\n",
    "    misfit_engine=\"r2\",\n",
    "    window_seconds=5.0,\n",
    "    min_stations=5,\n",
    "    sam_class=VSAM,\n",
    "    sam_metric=\"VT\",\n",
    "    debug=True,\n",
    ").build()\n",
    "\n",
    "# Generate variants (youâ€™ll need cfg_variants_from() from wrappers2.py or config.py)\n",
    "variants = cfg_variants_from(baseline_cfg=cfg)\n",
    "\n",
    "REFINE_SECTOR = False\n",
    "summaries = []\n",
    "\n",
    "for i, ev in zip(best_file_nums, best_event_files):\n",
    "    ev_key = Path(ev).stem\n",
    "    print(f\"\\n[{i}/{len(event_files)}] Processing event: {ev_key}\")\n",
    "\n",
    "    for k, variant in variants.items():\n",
    "        try:\n",
    "            result = run_single_event(\n",
    "                mseed_file=str(ev),\n",
    "                cfg=variant,\n",
    "                refine_sector=REFINE_SECTOR,\n",
    "                station_gains_df=None,\n",
    "                switch_event_ctag=True,\n",
    "                topo_kw=topo_kw,\n",
    "                mseed_units=\"m/s\",\n",
    "                reduce_time=True,\n",
    "                debug=True,\n",
    "            )\n",
    "            summaries.append(result)\n",
    "            print(f\"  âœ“ Variant {k} finished successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âœ— Variant {k} failed: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01d7172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# --- tiny geo helper ---\n",
    "def _gc_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    p1 = math.radians(lat1); p2 = math.radians(lat2)\n",
    "    dphi = p2 - p1\n",
    "    dlmb = math.radians(lon2 - lon1)\n",
    "    a = math.sin(dphi/2)**2 + math.cos(p1)*math.cos(p2)*math.sin(dlmb/2)**2\n",
    "    return 2*R*math.asin(min(1.0, math.sqrt(a)))\n",
    "\n",
    "def _load_run_csv(path: str | Path):\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        raise IOError(f\"{path} does not exist\")\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # case-insensitive column map\n",
    "    cmap = {c.lower(): c for c in df.columns}\n",
    "    def getcol(name, default=np.nan):\n",
    "        key = name.lower()\n",
    "        if key in cmap:\n",
    "            return df[cmap[key]].to_numpy()\n",
    "        return np.full(len(df), default)\n",
    "\n",
    "    # parse time (t/time/utc/timestamp), normalize to UTC, round to seconds\n",
    "    time_cols = [c for c in (\"t\", \"time\", \"utc\", \"timestamp\") if c in cmap]\n",
    "    if time_cols:\n",
    "        raw = df[cmap[time_cols[0]]]\n",
    "        t_parsed = pd.to_datetime(raw, utc=True, errors=\"coerce\")\n",
    "        # count real timestamps\n",
    "        n_real = t_parsed.notna().sum()\n",
    "        if n_real > 0:\n",
    "            t = t_parsed.dt.round(\"S\").to_numpy(\"datetime64[ns]\")\n",
    "            time_kind = \"real\"\n",
    "        else:\n",
    "            t = np.arange(len(df))\n",
    "            time_kind = \"index\"\n",
    "    else:\n",
    "        t = np.arange(len(df))\n",
    "        time_kind = \"index\"\n",
    "\n",
    "    out = {\n",
    "        \"t\": t,\n",
    "        \"time_kind\": time_kind,\n",
    "        \"lat\": getcol(\"lat\").astype(float),\n",
    "        \"lon\": getcol(\"lon\").astype(float),\n",
    "        \"DR\": getcol(\"DR\").astype(float) if \"dr\" in cmap else getcol(\"dr\").astype(float),\n",
    "        \"misfit\": getcol(\"misfit\").astype(float),\n",
    "        \"azgap\": getcol(\"azgap\").astype(float),\n",
    "        \"connectedness\": float(np.nanmean(getcol(\"connectedness\"))) if \"connectedness\" in cmap else np.nan,\n",
    "        \"tag\": path.stem,\n",
    "    }\n",
    "    return out\n",
    "\n",
    "# --- aligner that falls back to index if time-intersection is empty ---\n",
    "def _align_two(A, B, return_common: bool = False):\n",
    "    \"\"\"\n",
    "    Align two runs:\n",
    "      - If both have real timestamps, intersect on rounded-to-second time.\n",
    "      - If no overlap on time (or one/both lack times), fall back to index alignment.\n",
    "    Returns (A2, B2) or (A2, B2, common) if return_common=True.\n",
    "    \"\"\"\n",
    "    def take(D, idx, common_vals):\n",
    "        T = {}\n",
    "        for k, v in D.items():\n",
    "            if isinstance(v, np.ndarray) and v.shape[0] == D[\"t\"].shape[0]:\n",
    "                T[k] = v[idx]\n",
    "            else:\n",
    "                T[k] = v\n",
    "        T[\"t\"] = common_vals\n",
    "        return T\n",
    "\n",
    "    used_time = False\n",
    "    if A[\"time_kind\"] == \"real\" and B[\"time_kind\"] == \"real\":\n",
    "        # normalize/round to seconds for intersection\n",
    "        tA = A[\"t\"].astype(\"datetime64[s]\").astype(\"datetime64[ns]\")\n",
    "        tB = B[\"t\"].astype(\"datetime64[s]\").astype(\"datetime64[ns]\")\n",
    "        common = np.intersect1d(tA, tB)\n",
    "        if common.size > 0:\n",
    "            idxA = {v: i for i, v in enumerate(tA)}\n",
    "            idxB = {v: i for i, v in enumerate(tB)}\n",
    "            iA = np.array([idxA[v] for v in common], dtype=int)\n",
    "            iB = np.array([idxB[v] for v in common], dtype=int)\n",
    "            used_time = True\n",
    "        else:\n",
    "            # fall back to index alignment\n",
    "            n = min(A[\"t\"].shape[0], B[\"t\"].shape[0])\n",
    "            if n <= 0:\n",
    "                return None\n",
    "            iA = np.arange(n, dtype=int)\n",
    "            iB = np.arange(n, dtype=int)\n",
    "            common = iA  # synthetic index\n",
    "    else:\n",
    "        # index alignment\n",
    "        n = min(A[\"t\"].shape[0], B[\"t\"].shape[0])\n",
    "        if n <= 0:\n",
    "            return None\n",
    "        iA = np.arange(n, dtype=int)\n",
    "        iB = np.arange(n, dtype=int)\n",
    "        common = iA\n",
    "\n",
    "    A2 = take(A, iA, common)\n",
    "    B2 = take(B, iB, common)\n",
    "    if return_common:\n",
    "        return A2, B2, common, (\"time\" if used_time else \"index\")\n",
    "    return A2, B2\n",
    "\n",
    "def compare_two_runs_csv(csvA, csvB, label=\"(baseline vs alt)\"):\n",
    "    if not csvA.is_file:\n",
    "        print(f'{csvA} does not exist')\n",
    "        return None\n",
    "    if not csvB.is_file:\n",
    "        print(f'{csvB} does not exist')\n",
    "        return None\n",
    "    print('both CSV files exist')\n",
    "    def _debug_run_info(label, R):\n",
    "        print(f\"[{label}] kind={R['time_kind']}, n={R['t'].shape[0]}\")\n",
    "        if R[\"time_kind\"] == \"real\" and R[\"t\"].size:\n",
    "            print(\"   first:\", R[\"t\"][0], \" last:\", R[\"t\"][-1])\n",
    "\n",
    "    # In compare_two_runs_csv just after loading:\n",
    "    try:\n",
    "        A = _load_run_csv(csvA); \n",
    "        B = _load_run_csv(csvB); \n",
    "    except:\n",
    "        return None\n",
    "    try:\n",
    "        aligned = _align_two(A, B)\n",
    "    except:\n",
    "        _debug_run_info(\"A\", A)\n",
    "        _debug_run_info(\"B\", B)\n",
    "    if aligned is None:\n",
    "        raise ValueError(\"No overlapping samples (time or index) between runs.\")\n",
    "    A, B = aligned\n",
    "\n",
    "    latA, lonA = A[\"lat\"], A[\"lon\"]\n",
    "    latB, lonB = B[\"lat\"], B[\"lon\"]\n",
    "\n",
    "    # amplitude-based weights (mean DR of the two runs)\n",
    "    DRw = np.nanmean(np.vstack([A[\"DR\"], B[\"DR\"]]), axis=0)\n",
    "    DRw = np.where(np.isfinite(DRw), DRw, 0.0)\n",
    "    w = DRw / (DRw.sum() + 1e-12) if DRw.max() > 0 else np.ones_like(DRw) / max(1, DRw.size)\n",
    "\n",
    "    # per-sample spatial separation\n",
    "    sep = np.array([\n",
    "        _gc_km(latA[i], lonA[i], latB[i], lonB[i])\n",
    "        if np.isfinite(latA[i]) and np.isfinite(lonA[i]) and np.isfinite(latB[i]) and np.isfinite(lonB[i])\n",
    "        else np.nan\n",
    "        for i in range(len(latA))\n",
    "    ], dtype=float)\n",
    "\n",
    "    mask = np.isfinite(sep)\n",
    "    mean_sep   = float(np.nanmean(sep[mask])) if mask.any() else np.nan\n",
    "    median_sep = float(np.nanmedian(sep[mask])) if mask.any() else np.nan\n",
    "    wmean_sep  = float(np.nansum(sep * w)) if mask.any() else np.nan\n",
    "\n",
    "    # simple % within thresholds\n",
    "    pct_1km = float(100.0 * np.nanmean(sep <= 1.0)) if mask.any() else np.nan\n",
    "    pct_2km = float(100.0 * np.nanmean(sep <= 2.0)) if mask.any() else np.nan\n",
    "    pct_5km = float(100.0 * np.nanmean(sep <= 5.0)) if mask.any() else np.nan\n",
    "\n",
    "    # (optional) path correlation on lat/lon by index overlap\n",
    "    m = np.isfinite(latA) & np.isfinite(lonA) & np.isfinite(latB) & np.isfinite(lonB)\n",
    "    lat_r = float(np.corrcoef(latA[m], latB[m])[0,1]) if np.count_nonzero(m) > 3 else np.nan\n",
    "    lon_r = float(np.corrcoef(lonA[m], lonB[m])[0,1]) if np.count_nonzero(m) > 3 else np.nan\n",
    "\n",
    "    # misfit / azgap averages and deltas\n",
    "    mean_misfit_A = float(np.nanmean(A[\"misfit\"]))\n",
    "    mean_misfit_B = float(np.nanmean(B[\"misfit\"]))\n",
    "    d_misfit = mean_misfit_B - mean_misfit_A\n",
    "\n",
    "    mean_azgap_A = float(np.nanmean(A[\"azgap\"]))\n",
    "    mean_azgap_B = float(np.nanmean(B[\"azgap\"]))\n",
    "    d_azgap = mean_azgap_B - mean_azgap_A\n",
    "\n",
    "    return {\n",
    "        \"runA_tag\": A[\"tag\"],\n",
    "        \"runB_tag\": B[\"tag\"],\n",
    "        \"label\": label,\n",
    "        \"align_mode\": \"time\" if (A[\"time_kind\"]==\"real\" and B[\"time_kind\"]==\"real\") else \"index\",\n",
    "        \"n_overlap\": int(A[\"t\"].shape[0]),\n",
    "        \"mean_sep_km\": mean_sep,\n",
    "        \"median_sep_km\": median_sep,\n",
    "        \"amp_weighted_mean_sep_km\": wmean_sep,\n",
    "        \"pct_within_1km\": pct_1km,\n",
    "        \"pct_within_2km\": pct_2km,\n",
    "        \"pct_within_5km\": pct_5km,\n",
    "        \"lat_corr\": lat_r,\n",
    "        \"lon_corr\": lon_r,\n",
    "        \"mean_misfit_A\": mean_misfit_A,\n",
    "        \"mean_misfit_B\": mean_misfit_B,\n",
    "        \"delta_misfit_B_minus_A\": d_misfit,\n",
    "        \"mean_azgap_A\": mean_azgap_A,\n",
    "        \"mean_azgap_B\": mean_azgap_B,\n",
    "        \"delta_azgap_B_minus_A\": d_azgap,\n",
    "        \"connectedness_A\": A[\"connectedness\"],\n",
    "        \"connectedness_B\": B[\"connectedness\"],\n",
    "        \"delta_connectedness_B_minus_A\": float(B[\"connectedness\"] - A[\"connectedness\"]) \\\n",
    "            if (np.isfinite(B[\"connectedness\"]) and np.isfinite(A[\"connectedness\"])) else np.nan,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def safe_compare(summary_csv, csvA, csvB, label):\n",
    "    csvA, csvB = Path(csvA), Path(csvB)\n",
    "    if not csvA.exists():\n",
    "        print(f\"[skip] missing baseline: {csvA}\")\n",
    "        return None\n",
    "    if not csvB.exists():\n",
    "        print(f\"[skip] missing variant:  {csvB}\")\n",
    "        return None\n",
    "    try:\n",
    "        row = compare_two_runs_csv(csvA, csvB, label)\n",
    "    except Exception as e:\n",
    "        print(f\"[skip] compare failed ({label}): {e}\")\n",
    "        return None\n",
    "    out = Path(summary_csv)\n",
    "    df = pd.DataFrame([row])\n",
    "    if out.exists():\n",
    "        df0 = pd.read_csv(out)\n",
    "        df = pd.concat([df0, df], ignore_index=True)\n",
    "    df.to_csv(out, index=False)\n",
    "    print(f\"[compare] appended to {out} ({row['align_mode']} alignment, n={row['n_overlap']})\")\n",
    "    return row\n",
    "\n",
    "for i, ev in zip(best_file_nums, best_event_files):\n",
    "    print(f\"[{i}/{len(event_files)}] {ev}\")\n",
    "    mseed_file=str(ev)\n",
    "\n",
    "    # Build paths for baseline + variants for ONE event:\n",
    "    event_dir    = OUTPUT_DIR / Path(mseed_file).stem\n",
    "    products_dir = event_dir / Path(cfg.outdir).name\n",
    "\n",
    "    baseline = products_dir / \"source_VSAM_VT_5s_body_v3_Q100_F2_3d_r2_SC.csv\"\n",
    "    alt_Q23  = event_dir / \"VSAM_VT_5s_body_v3_Q23_F2_3d_r2_SC\" / \"source_VSAM_VT_5s_body_v3_Q23_F2_3d_r2_SC.csv\"\n",
    "    alt_v15  = event_dir / \"VSAM_VT_5s_body_v1.5_Q100_F2_3d_r2_SC\" / \"source_VSAM_VT_5s_body_v1.5_Q100_F2_3d_r2_SC.csv\"\n",
    "    alt_win2 = event_dir / \"VSAM_VT_2s_body_v3_Q100_F2_3d_r2_SC\" / \"source_VSAM_VT_2s_body_v3_Q100_F2_3d_r2_SC.csv\"\n",
    "    alt_mean = event_dir / \"VSAM_mean_5s_body_v3_Q100_F2_3d_r2_SC\" / \"source_VSAM_mean_5s_body_v3_Q100_F2_3d_r2_SC.csv\"\n",
    "    alt_nosc = event_dir / \"VSAM_VT_5s_body_v3_Q100_F2_3d_r2\"  / \"source_VSAM_VT_5s_body_v3_Q100_F2_3d_r2.csv\"# station corr OFF\n",
    "\n",
    "    summary_csv = event_dir / \"pairwise_run_comparisons.csv\"\n",
    "\n",
    "    # Append comparisons (CSV vs CSV; aligns by time if present, else by index)\n",
    "    safe_compare(summary_csv, baseline, alt_Q23,  label=\"Q 100â†’23\")\n",
    "    safe_compare(summary_csv, baseline, alt_v15,  label=\"speed 3.0â†’1.5 km/s\")\n",
    "    #safe_compare(summary_csv, baseline, alt_win2, label=\"window 5â†’2 s\")\n",
    "    safe_compare(summary_csv, baseline, alt_mean, label=\"metric VTâ†’mean\")\n",
    "    safe_compare(summary_csv, baseline, alt_nosc, label=\"station corr ONâ†’OFF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198a0459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run ASL per event (cell 6) ---\n",
    "'''\n",
    "from typing import List, Dict, Any\n",
    "summaries: List[Dict[str, Any]] = []\n",
    "\n",
    "for i, ev in zip(best_file_nums, best_event_files):\n",
    "    print(f\"[{i}/{len(event_files)}] {ev}\")\n",
    "    result = run_single_event(\n",
    "        mseed_file=str(ev),\n",
    "        cfg=cfg,\n",
    "        refine_sector=REFINE_SECTOR,\n",
    "        station_gains_df=None,\n",
    "        topo_kw=topo_kw,\n",
    "        debug=True,\n",
    "    )\n",
    "    summaries.append(result)\n",
    "    break\n",
    "\n",
    "# Summarize\n",
    "df = pd.DataFrame(summaries)\n",
    "display(df)\n",
    "\n",
    "summary_csv = Path(OUTPUT_DIR) / f\"{cfg.tag()}__summary.csv\"\n",
    "df.to_csv(summary_csv, index=False)\n",
    "print(f\"Summary saved to: {summary_csv}\")\n",
    "\n",
    "if not df.empty:\n",
    "    n_ok = int((~df.get(\"error\").notna()).sum()) if \"error\" in df.columns else len(df)\n",
    "    print(f\"Success: {n_ok}/{len(df)}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39632ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_all_event_comparisons(root: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Crawl event folders under `root` and stack `pairwise_run_comparisons.csv`.\n",
    "    Returns a tidy DF with event_id inferred from folder name.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for csv in root.rglob(\"pairwise_run_comparisons.csv\"):\n",
    "        try:\n",
    "            df = pd.read_csv(csv)\n",
    "            df[\"event_id\"] = csv.parent.name            # the event folder name\n",
    "            rows.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"[skip] {csv}: {e}\")\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "    out = pd.concat(rows, ignore_index=True)\n",
    "    # normalize label text to a short key\n",
    "    out[\"variant\"] = out[\"label\"].astype(str)\n",
    "    # guard presence of expected columns\n",
    "    for c in [\"mean_sep_km\",\"delta_misfit_B_minus_A\",\"delta_azgap_B_minus_A\"]:\n",
    "        if c not in out.columns: out[c] = np.nan\n",
    "    return out\n",
    "\n",
    "def add_composite_score(df: pd.DataFrame,\n",
    "                        w_sep=1.0, w_misfit=0.5, w_azgap=0.1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Lower is better. Negative deltas are good if they reduce misfit/azgap.\n",
    "    \"\"\"\n",
    "    d = df.copy()\n",
    "    # z-score each metric for comparability (event-wise optional)\n",
    "    # here: global z-scores; switch to per-event z if events differ strongly in scale\n",
    "    for col in [\"mean_sep_km\",\"delta_misfit_B_minus_A\",\"delta_azgap_B_minus_A\"]:\n",
    "        x = d[col].to_numpy(dtype=float)\n",
    "        mu, sd = np.nanmean(x), np.nanstd(x) if np.nanstd(x)>0 else 1.0\n",
    "        d[col+\"_z\"] = (x - mu)/sd\n",
    "    d[\"score\"] = (\n",
    "        w_sep    * d[\"mean_sep_km_z\"] +\n",
    "        w_misfit * d[\"delta_misfit_B_minus_A_z\"] +\n",
    "        w_azgap  * d[\"delta_azgap_B_minus_A_z\"]\n",
    "    )\n",
    "    return d\n",
    "\n",
    "def summarize_variants(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    One line per variant: meanÂ±SE of core metrics and composite score,\n",
    "    plus 'wins' (how often variant beats baseline the most for an event).\n",
    "    \"\"\"\n",
    "    g = df.groupby(\"variant\", dropna=False)\n",
    "    agg = g.agg(\n",
    "        n_events          = (\"event_id\", \"nunique\"),\n",
    "        n_rows            = (\"event_id\", \"size\"),\n",
    "        mean_sep_km_mean  = (\"mean_sep_km\", \"mean\"),\n",
    "        mean_sep_km_med   = (\"mean_sep_km\", \"median\"),\n",
    "        mean_sep_km_se    = (\"mean_sep_km\", lambda x: np.nanstd(x)/np.sqrt(max(1,(x.notna().sum())))),\n",
    "        dmisfit_mean      = (\"delta_misfit_B_minus_A\", \"mean\"),\n",
    "        dmisfit_med       = (\"delta_misfit_B_minus_A\", \"median\"),\n",
    "        dazgap_mean       = (\"delta_azgap_B_minus_A\", \"mean\"),\n",
    "        score_mean        = (\"score\", \"mean\"),\n",
    "        score_med         = (\"score\", \"median\"),\n",
    "    ).reset_index().sort_values(\"score_mean\")\n",
    "    return agg\n",
    "\n",
    "def per_event_winner(df_scored: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each event, pick the variant with the lowest composite score.\n",
    "    \"\"\"\n",
    "    # keep only the best per (event_id)\n",
    "    idx = df_scored.groupby(\"event_id\")[\"score\"].idxmin()\n",
    "    winners = df_scored.loc[idx, [\"event_id\",\"variant\",\"score\"]]\n",
    "    win_counts = winners.groupby(\"variant\").size().rename(\"wins\").reset_index()\n",
    "    return winners, win_counts.sort_values(\"wins\", ascending=False)\n",
    "\n",
    "# --- run it ---\n",
    "ROOT = OUTPUT_DIR  # your existing OUTDIR base\n",
    "allcmp = load_all_event_comparisons(ROOT)\n",
    "print(f\"stacked rows: {len(allcmp)}, events: {allcmp['event_id'].nunique()}\")\n",
    "\n",
    "scored = add_composite_score(allcmp, w_sep=1.0, w_misfit=0.5, w_azgap=0.1)\n",
    "summary = summarize_variants(scored)\n",
    "winners, win_counts = per_event_winner(scored)\n",
    "\n",
    "# quick looks\n",
    "display(summary.head(10))\n",
    "display(win_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a47df7",
   "metadata": {},
   "source": [
    "# Run all events efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552236da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(INPUT_DIR)\n",
    "print(cfg)\n",
    "print(topo_kw)\n",
    "print(REFINE_SECTOR)\n",
    "'''\n",
    "run_all_events(\n",
    "    input_dir=INPUT_DIR,\n",
    "    station_gains_df = None,\n",
    "    cfg=cfg,\n",
    "    refine_sector=REFINE_SECTOR,\n",
    "    topo_kw=topo_kw,\n",
    "    debug=True,\n",
    "    max_events=999999,\n",
    "    use_multiprocessing=True,\n",
    "    workers=4,\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613e3b64",
   "metadata": {},
   "source": [
    "# Run Monte Carlo sweep of parameters for 1 event\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13b73fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from flovopy.asl.wrappers2 import run_event_monte_carlo\n",
    "from flovopy.processing.sam import VSAM, DSAM\n",
    "'''\n",
    "# Simple 6-draw sweep (replace with your own priors/sequences)\n",
    "configs = ASLConfig.generate_config_list(\n",
    "    inventory=None,\n",
    "    output_base=None,\n",
    "    gridobj=None,\n",
    "    global_cache=None,      \n",
    "    wave_kinds=(\"surface\",\"body\"),\n",
    "    station_corr_tables=(station_corrections_df), #annual_station_corrections_df),\n",
    "    speeds=(1.0, 3.0),\n",
    "    Qs=(23, 1000),\n",
    "    dist_modes=(\"3d\",), # 2d needs a different grid and different distance and amplitude corrections\n",
    "    misfit_engines=(\"l2\",\"r2\", \"lin\"),\n",
    "    peakfs=(2.0, 8.0),\n",
    "    window_seconds = 5.0, # change to be a tuple 10.0) not implemented yet\n",
    "    min_stations = 5,\n",
    "    sam_class = (VSAM), #, DSAM), # not implemented yet\n",
    "    sam_metric = (\"mean\"),# \"median\", \"rms\", \"VT\", \"LP\"), # this doesn't seem to be implemented yet\n",
    "    # context can be set later; set here if you like:\n",
    "    debug=False,\n",
    ")\n",
    "\n",
    "configs = ASLConfig.generate_config_list(\n",
    "    inventory=None,\n",
    "    output_base=None,\n",
    "    gridobj=None,\n",
    "    global_cache=None,      \n",
    "    wave_kinds=(\"surface\",),\n",
    "    station_corr_tables=(station_corrections_df), #annual_station_corrections_df),\n",
    "    speeds=(1.0, 3.0),\n",
    "    Qs=(23, 1000),\n",
    "    dist_modes=(\"3d\",), # 2d needs a different grid and different distance and amplitude corrections\n",
    "    misfit_engines=(\"l2\"),\n",
    "    peakfs=(8.0),\n",
    "    window_seconds = 5.0, # change to be a tuple 10.0) not implemented yet\n",
    "    min_stations = 5,\n",
    "    sam_class = (VSAM), #, DSAM), # not implemented yet\n",
    "    sam_metric = (\"mean\"),# \"median\", \"rms\", \"VT\", \"LP\"), # this doesn't seem to be implemented yet\n",
    "    # context can be set later; set here if you like:\n",
    "    debug=False,\n",
    ")\n",
    "\n",
    "\n",
    "configs = ASLConfig.generate_config_list(    \n",
    "    inventory=INV,\n",
    "    output_base=str(OUTPUT_DIR),\n",
    "    gridobj=gridobj,\n",
    "    global_cache=GLOBAL_CACHE,\n",
    ") \n",
    "\n",
    "print(len(configs))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21faaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Shared run context\n",
    "mseed_file   = event_files[116]\n",
    "'''\n",
    "results = run_event_monte_carlo(\n",
    "    mseed_file=mseed_file,\n",
    "    configs=configs,\n",
    "    inventory=INV,\n",
    "    output_base=str(OUTPUT_DIR),\n",
    "    gridobj=gridobj,\n",
    "    topo_kw=topo_kw,\n",
    "    station_gains_df=None,\n",
    "    parallel=False,\n",
    "    max_workers=1,\n",
    "    global_cache=GLOBAL_CACHE,\n",
    "    debug=True,\n",
    ")\n",
    "\n",
    "# Inspect or summarize results as needed\n",
    "n_ok = sum(1 for r in results if \"error\" not in r)\n",
    "print(f\"[MC] Completed {n_ok}/{len(results)} runs OK\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flovopy_plus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
