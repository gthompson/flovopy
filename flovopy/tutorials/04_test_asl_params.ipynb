{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6ce9277",
   "metadata": {},
   "source": [
    "# 1. Set up parameters for ASL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94679f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from obspy import read_inventory\n",
    "from importlib import reload\n",
    "from flovopy.asl.wrappers2 import run_single_event, find_event_files, run_all_events\n",
    "from flovopy.core.mvo import dome_location, REGION_DEFAULT\n",
    "from flovopy.processing.sam import VSAM, DSAM \n",
    "from flovopy.asl.config import ASLConfig\n",
    "# -------------------------- Config --------------------------\n",
    "# directories\n",
    "HOME = Path.home()\n",
    "PROJECTDIR      = HOME / \"Dropbox\" / \"BRIEFCASE\" / \"SSADenver\"\n",
    "LOCALPROJECTDIR = HOME / \"work\" / \"PROJECTS\" / \"SSADenver_local\"\n",
    "OUTPUT_DIR      = LOCALPROJECTDIR / \"asl_results\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "INPUT_DIR       = PROJECTDIR / \"ASL_inputs\" / \"biggest_pdc_events\"\n",
    "GLOBAL_CACHE    = PROJECTDIR / \"asl_global_cache\"\n",
    "METADATA_DIR    = PROJECTDIR / \"metadata\" \n",
    "STATION_CORRECTIONS_DIR = PROJECTDIR / \"station_correction_analysis\"\n",
    "\n",
    "# master files\n",
    "INVENTORY_XML   = METADATA_DIR / \"MV_Seismic_and_GPS_stations.xml\"\n",
    "DEM_DEFAULT     = METADATA_DIR / \"MONTSERRAT_DEM_WGS84_MASTER.tif\"\n",
    "GRIDFILE_DEFAULT= METADATA_DIR / \"MASTER_GRID_MONTSERRAT.pkl\"\n",
    "\n",
    "# parameters for envelopes and cross-correlation\n",
    "SMOOTH_SECONDS  = 1.0\n",
    "MAX_LAG_SECONDS = 8.0\n",
    "MIN_XCORR       = 0.5\n",
    "\n",
    "# other parameters\n",
    "DIST_MODE = \"3d\" # or 2d. will essentially squash Montserrat topography and stations onto a sea-level plane, ignored elevation data, e.g. for computing distances\n",
    "\n",
    "# Inventory of Montserrat stations\n",
    "from obspy import read_inventory\n",
    "INV     = read_inventory(INVENTORY_XML)\n",
    "print(f\"[INV] Networks: {len(INV)}  Stations: {sum(len(n) for n in INV)}  Channels: {sum(len(sta) for net in INV for sta in net)}\")\n",
    "\n",
    "# Montserrat station corrections estimated from regionals\n",
    "station_corrections_csv = STATION_CORRECTIONS_DIR / \"station_gains_intervals.csv\"\n",
    "annual_station_corrections_csv = STATION_CORRECTIONS_DIR / \"station_gains_intervals_by_year.csv\"\n",
    "station_corrections_df = pd.read_csv(station_corrections_csv)\n",
    "annual_station_corrections_df = pd.read_csv(annual_station_corrections_csv)\n",
    "\n",
    "# Montserrat pre-defined Grid (from 02 tutorial)\n",
    "from flovopy.asl.grid import Grid\n",
    "gridobj = Grid.load(GRIDFILE_DEFAULT)\n",
    "print(gridobj)\n",
    "\n",
    "\n",
    "# Montserrat constants\n",
    "from flovopy.core.mvo import dome_location, REGION_DEFAULT\n",
    "print(\"Dome (assumed source) =\", dome_location)\n",
    "\n",
    "# events and wrappers\n",
    "from flovopy.asl.wrappers2 import run_single_event, find_event_files, run_all_events\n",
    "event_files = list(find_event_files(INPUT_DIR))\n",
    "eventcsvfile = Path(OUTPUT_DIR) / \"mseed_files.csv\"\n",
    "if not eventcsvfile.is_file():\n",
    "    rows = [{\"num\": num, \"f\": str(f)} for num, f in enumerate(event_files)]\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(eventcsvfile, index=False)\n",
    "best_file_nums  = [35, 36, 40, 52, 82, 83, 84, 116, 310, 338]\n",
    "best_event_files = [event_files[i] for i in best_file_nums]\n",
    "print(f'Best miniseed files are: {best_event_files}')\n",
    "REFINE_SECTOR = False   # enable triangular dome-to-sea refinement\n",
    "\n",
    "# Parameters to pass for making pygmt topo maps\n",
    "topo_kw = {\n",
    "    \"inv\": INV,\n",
    "    \"add_labels\": True,\n",
    "    \"cmap\": \"gray\",\n",
    "    \"region\": REGION_DEFAULT,\n",
    "    \"dem_tif\": DEM_DEFAULT,  # basemap shading from your GeoTIFF - but does not actually seem to use this unless topo_color=True and cmap=None\n",
    "    \"frame\": True,\n",
    "    \"dome_location\": dome_location,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd2c7cd",
   "metadata": {},
   "source": [
    "# Run events from last cell, one event at a time, to check it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed3982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an ASL Configuration. This is inherited by various downstream functions\n",
    "# This describes the physical parameters, the station metadata, the grid, the misfit algorithm, etc.\n",
    "cfg = ASLConfig(\n",
    "    inventory=INV,\n",
    "    output_base=OUTPUT_DIR, # str?\n",
    "    gridobj=gridobj,\n",
    "    global_cache=GLOBAL_CACHE,\n",
    "    station_correction_dataframe=station_corrections_df,\n",
    "    wave_kind = \"body\", # \"surface\" or \"body\"\n",
    "    speed = 3.0, # km/s\n",
    "    Q = 100, # attenuation quality factor\n",
    "    peakf = 2.0, # Hz\n",
    "    dist_mode = DIST_MODE, # or \"2d\"\n",
    "    misfit_engine = \"r2\", # l2, r2, lin?\n",
    "    window_seconds = 5.0, # length of time window for amplitude measurement\n",
    "    min_stations = 5, # minimum number of stations required to locate event\n",
    "    sam_class = VSAM, # or DSAM\n",
    "    sam_metric = \"VT\", # or one of \"mean\", \"median\", \"max\", \"rms\", \"VLP\", or \"LP\"\n",
    "    debug=True,\n",
    ")\n",
    "cfg.build()\n",
    "summaries = []\n",
    "\n",
    "REFINE_SECTOR=False\n",
    "for i, ev in zip(best_file_nums, best_event_files):\n",
    "    print(f\"[{i}/{len(event_files)}] {ev}\")\n",
    "    result = run_single_event(\n",
    "        mseed_file=str(ev),\n",
    "        cfg=cfg,\n",
    "        refine_sector=REFINE_SECTOR,\n",
    "        station_gains_df=None,\n",
    "        switch_event_ctag = True,\n",
    "        topo_kw=topo_kw,\n",
    "        mseed_units='m/s', # default units for miniseed files being used - probably \"Counts\" or \"m/s\"        \n",
    "        reduce_time=True,\n",
    "        debug=True,\n",
    "    )\n",
    "    summaries.append(result)\n",
    "\n",
    "# Summarize\n",
    "df = pd.DataFrame(summaries)\n",
    "display(df)\n",
    "\n",
    "summary_csv = Path(OUTPUT_DIR) / f\"{cfg.tag()}__summary.csv\"\n",
    "df.to_csv(summary_csv, index=False)\n",
    "print(f\"Summary saved to: {summary_csv}\")\n",
    "\n",
    "if not df.empty:\n",
    "    n_ok = int((~df.get(\"error\").notna()).sum()) if \"error\" in df.columns else len(df)\n",
    "    print(f\"Success: {n_ok}/{len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093c30a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Minimal orchestrator: build variants, resolve CSVs, append comparisons ---\n",
    "\n",
    "from dataclasses import replace\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import the real compare helpers, but alias safe_compare to avoid name clash\n",
    "from flovopy.asl.analyze_run_pairs import (\n",
    "    safe_compare as append_compare,\n",
    "    load_all_event_comparisons,\n",
    "    add_composite_score,\n",
    "    summarize_variants,\n",
    "    per_event_winner,\n",
    ")\n",
    "\n",
    "# ---------- 2) One-change variants ----------\n",
    "def cfg_variants_from(baseline: ASLConfig) -> dict[str, ASLConfig]:\n",
    "    return {\n",
    "        \"Q100\":             replace(baseline, Q=100).build(),\n",
    "        \"Q10\":              replace(baseline, Q=10).build(),\n",
    "        \"v1.0\":             replace(baseline, speed=1.0).build(),\n",
    "        \"v3.0\":             replace(baseline, speed=3.0).build(),\n",
    "        \"win1s\":            replace(baseline, window_seconds=1.0).build(),\n",
    "        \"win10s\":           replace(baseline, window_seconds=10.0).build(),\n",
    "        \"metric_median\":    replace(baseline, sam_metric=\"median\").build(),\n",
    "        \"metric_LP\":        replace(baseline, sam_metric=\"LP\").build(),\n",
    "        \"metric_VT\":        replace(baseline, sam_metric=\"VT\").build(),\n",
    "        \"no_stacorr\":       replace(baseline, station_correction_dataframe=None).build(),\n",
    "        \"annual_stacorr\":   replace(baseline, station_correction_dataframe=annual_station_corrections_df).build(),\n",
    "        \"l2_engine\":        replace(baseline, misfit_engine=\"l2\").build(),\n",
    "        \"lin_engine\":       replace(baseline, misfit_engine=\"lin\").build(),\n",
    "        \"body\":             replace(baseline, wave_kind=\"body\").build(), # change speed too?\n",
    "        \"f5hz\":             replace(baseline, peakf=5.0).build(),\n",
    "        \"f8hz\":             replace(baseline, peakf=8.0).build(),\n",
    "        \"2d\":               replace(baseline, dist_mode='2d').build(),\n",
    "        \"landgrid\":         replace(baseline, gridobj=landgridobj).build(), \n",
    "    }\n",
    "\n",
    "REFINE_SECTOR = False\n",
    "landgridobj = Grid.load(GLOBAL_CACHE / \"land\" / \"Grid_9c2fd59b.pkl\")\n",
    "\n",
    "\n",
    "# ---------- 1) Build baseline config ----------\n",
    "baseline_cfg = ASLConfig(\n",
    "    inventory=INV,\n",
    "    output_base=OUTPUT_DIR,\n",
    "    gridobj=gridobj,\n",
    "    global_cache=GLOBAL_CACHE,\n",
    "    station_correction_dataframe=station_corrections_df,\n",
    "    wave_kind=\"surface\",\n",
    "    speed=1.5,\n",
    "    Q=23,\n",
    "    peakf=2.0,\n",
    "    dist_mode=\"3d\", \n",
    "    misfit_engine=\"r2\",\n",
    "    window_seconds=5.0,\n",
    "    min_stations=5,\n",
    "    sam_class=VSAM,\n",
    "    sam_metric=\"mean\",\n",
    "    debug=False,\n",
    ").build()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "variants = cfg_variants_from(baseline_cfg)\n",
    "\n",
    "# ---------- 3) Locate each run’s CSV by tag ----------\n",
    "def _products_dir_for(cfg: ASLConfig, mseed_file: str | Path) -> Path:\n",
    "    mseed_file = Path(mseed_file)\n",
    "    event_dir = Path(cfg.output_base) / mseed_file.stem\n",
    "    return event_dir / Path(cfg.outdir).name\n",
    "\n",
    "def csv_for_run(cfg: ASLConfig, mseed_file: str | Path) -> Path | None:\n",
    "    pdir = _products_dir_for(cfg, mseed_file)\n",
    "    tag = cfg.tag()\n",
    "    candidates = [\n",
    "        pdir / f\"source_{tag}_refined.csv\",\n",
    "        pdir / f\"source_{tag}.csv\",\n",
    "        pdir / f\"{tag}_refined.csv\",\n",
    "        pdir / f\"{tag}.csv\",\n",
    "    ]\n",
    "    for c in candidates:\n",
    "        if c.exists():\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "# (Optional) auto-run a variant if its CSV is missing\n",
    "RUN_IF_MISSING_BASELINE = True      # auto-run baseline if its CSV is missing\n",
    "RUN_IF_MISSING_VARIANTS = True     # keep variants manual (or flip to True if you want)\n",
    "\n",
    "def ensure_csv_for(cfg: ASLConfig, mseed_file: str | Path, *, run_if_missing: bool | None = None) -> Path | None:\n",
    "    \"\"\"Return path to run CSV, optionally auto-running if missing.\"\"\"\n",
    "    run_flag = RUN_IF_MISSING if run_if_missing is None else run_if_missing  # keeps backward-compat if you still use RUN_IF_MISSING elsewhere\n",
    "    csv = csv_for_run(cfg, mseed_file)\n",
    "    if (csv is None or not csv.exists()) and run_flag:\n",
    "        try:\n",
    "            _ = run_single_event(\n",
    "                mseed_file=str(mseed_file),\n",
    "                cfg=cfg,\n",
    "                refine_sector=REFINE_SECTOR,\n",
    "                station_gains_df=None,\n",
    "                switch_event_ctag=True,\n",
    "                topo_kw=topo_kw,\n",
    "                mseed_units=\"m/s\",\n",
    "                reduce_time=True,\n",
    "                debug=True,\n",
    "            )\n",
    "            csv = csv_for_run(cfg, mseed_file)\n",
    "        except Exception as e:\n",
    "            print(f\"  [run error] {Path(mseed_file).stem} · {cfg.tag()}: {e}\")\n",
    "            return None\n",
    "    return csv\n",
    "\n",
    "# ---------- 4) Compare baseline vs each variant ----------\n",
    "base_tag = baseline_cfg.tag()\n",
    "\n",
    "for i, ev in zip(best_file_nums, best_event_files):\n",
    "    ev_key = Path(ev).stem\n",
    "    print(f\"\\n[{i}/{len(event_files)}] {ev_key}\")\n",
    "    event_dir = Path(OUTPUT_DIR) / ev_key\n",
    "\n",
    "    # Encode the baseline tag so different baselines don't overwrite each other\n",
    "    summary_csv = event_dir / f\"pairwise_{baseline_cfg.tag()}_vs_variants.csv\"\n",
    "\n",
    "    # Baseline: auto-run only if missing\n",
    "    base_csv = ensure_csv_for(baseline_cfg, ev, run_if_missing=RUN_IF_MISSING_BASELINE)\n",
    "    if base_csv is None:\n",
    "        print(\"  [skip] no baseline CSV; cannot compare.\")\n",
    "        continue\n",
    "\n",
    "    # Variants: respect their own flag (default False here)\n",
    "    for key, vcfg in variants.items():\n",
    "        alt_csv = ensure_csv_for(vcfg, ev, run_if_missing=RUN_IF_MISSING_VARIANTS)\n",
    "        try:\n",
    "            # If you already removed label_map, pass a generated label here (e.g., vcfg.tag())\n",
    "            append_compare(summary_csv, base_csv, alt_csv, label=vcfg.tag())\n",
    "        except Exception as e:\n",
    "            print(f\"  [compare error] {vcfg.tag()}: {e}\")\n",
    "\n",
    "# ---------- 5) Roll-up ----------\n",
    "ROOT = OUTPUT_DIR\n",
    "allcmp = load_all_event_comparisons(ROOT)\n",
    "print(f\"\\nstacked rows: {len(allcmp)}, events: {allcmp['event_id'].nunique() if not allcmp.empty else 0}\")\n",
    "\n",
    "if not allcmp.empty:\n",
    "    scored  = add_composite_score(allcmp, w_sep=1.0, w_misfit=0.5, w_azgap=0.1)\n",
    "    summary = summarize_variants(scored)\n",
    "    winners, win_counts = per_event_winner(scored)\n",
    "    display(summary.head(10))\n",
    "    display(win_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2962d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Minimal orchestrator: build variants, resolve CSVs, append comparisons ---\n",
    "\n",
    "from dataclasses import replace\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from flovopy.asl.analyze_run_pairs import (\n",
    "    safe_compare as append_compare,\n",
    "    load_all_event_comparisons,\n",
    "    add_composite_score,\n",
    "    summarize_variants,\n",
    "    per_event_winner,\n",
    ")\n",
    "\n",
    "# ---------- 2) One-change variants ----------\n",
    "def cfg_variants_from(baseline: ASLConfig) -> dict[str, ASLConfig]:\n",
    "    return {\n",
    "        \"Q100\":             replace(baseline, Q=100).build(),\n",
    "        \"Q10\":              replace(baseline, Q=10).build(),\n",
    "        \"v1.0\":             replace(baseline, speed=1.0).build(),\n",
    "        \"v3.0\":             replace(baseline, speed=3.0).build(),\n",
    "        \"win1s\":            replace(baseline, window_seconds=1.0).build(),\n",
    "        \"win10s\":           replace(baseline, window_seconds=10.0).build(),\n",
    "        \"metric_median\":    replace(baseline, sam_metric=\"median\").build(),\n",
    "        \"metric_LP\":        replace(baseline, sam_metric=\"LP\").build(),\n",
    "        \"metric_VT\":        replace(baseline, sam_metric=\"VT\").build(),\n",
    "        \"no_stacorr\":       replace(baseline, station_correction_dataframe=None).build(),\n",
    "        \"annual_stacorr\":   replace(baseline, station_correction_dataframe=annual_station_corrections_df).build(),\n",
    "        \"l2_engine\":        replace(baseline, misfit_engine=\"l2\").build(),\n",
    "        \"lin_engine\":       replace(baseline, misfit_engine=\"lin\").build(),\n",
    "        \"body\":             replace(baseline, wave_kind=\"body\").build(),  # (optionally also change speed)\n",
    "        \"f5hz\":             replace(baseline, peakf=5.0).build(),\n",
    "        \"f8hz\":             replace(baseline, peakf=8.0).build(),\n",
    "        \"2d\":               replace(baseline, dist_mode=\"2d\").build(),\n",
    "        \"landgrid\":         replace(baseline, gridobj=landgridobj).build(),\n",
    "    }\n",
    "\n",
    "# ---------- 3) Locate each run’s CSV by tag ----------\n",
    "def _products_dir_for(cfg: ASLConfig, mseed_file: str | Path) -> Path:\n",
    "    mseed_file = Path(mseed_file)\n",
    "    event_dir = Path(cfg.output_base) / mseed_file.stem\n",
    "    return event_dir / Path(cfg.outdir).name\n",
    "\n",
    "def csv_for_run(cfg: ASLConfig, mseed_file: str | Path) -> Path | None:\n",
    "    pdir = _products_dir_for(cfg, mseed_file)\n",
    "    tag = cfg.tag()\n",
    "    candidates = [\n",
    "        pdir / f\"source_{tag}_refined.csv\",\n",
    "        pdir / f\"source_{tag}.csv\",\n",
    "        pdir / f\"{tag}_refined.csv\",\n",
    "        pdir / f\"{tag}.csv\",\n",
    "    ]\n",
    "    for c in candidates:\n",
    "        if c.exists():\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "# ---------- 3.5) Auto-run switches ----------\n",
    "def ensure_csv_for(cfg: ASLConfig, mseed_file: str | Path, *, is_baseline: bool | None = None) -> Path | None:\n",
    "    \"\"\"\n",
    "    Return path to run CSV, optionally auto-running if missing.\n",
    "    If is_baseline is None, infer by object identity against baseline_cfg.\n",
    "    \"\"\"\n",
    "    if is_baseline is None:\n",
    "        is_baseline = (cfg is baseline_cfg)\n",
    "    run_flag = RUN_IF_MISSING_BASELINE if is_baseline else RUN_IF_MISSING_VARIANTS\n",
    "\n",
    "    csv = csv_for_run(cfg, mseed_file)\n",
    "    if (csv is None or not csv.exists()) and run_flag:\n",
    "        try:\n",
    "            _ = run_single_event(\n",
    "                mseed_file=str(mseed_file),\n",
    "                cfg=cfg,\n",
    "                refine_sector=REFINE_SECTOR,\n",
    "                station_gains_df=None,\n",
    "                switch_event_ctag=True,\n",
    "                topo_kw=topo_kw,\n",
    "                mseed_units=\"m/s\",\n",
    "                reduce_time=True,\n",
    "                debug=True,\n",
    "            )\n",
    "            csv = csv_for_run(cfg, mseed_file)\n",
    "        except Exception as e:\n",
    "            print(f\"  [run error] {Path(mseed_file).stem} · {cfg.tag()}: {e}\")\n",
    "            return None\n",
    "    return csv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# RUN\n",
    "\n",
    "landgridobj = Grid.load(GLOBAL_CACHE / \"land\" / \"Grid_9c2fd59b.pkl\")\n",
    "REFINE_SECTOR = False\n",
    "\n",
    "# ---------- 1) Build baseline config ----------\n",
    "baseline_cfg = ASLConfig(\n",
    "    inventory=INV,\n",
    "    output_base=OUTPUT_DIR,\n",
    "    gridobj=gridobj,\n",
    "    global_cache=GLOBAL_CACHE,\n",
    "    station_correction_dataframe=station_corrections_df,\n",
    "    wave_kind=\"surface\",\n",
    "    speed=1.5,\n",
    "    Q=23,\n",
    "    peakf=2.0,\n",
    "    dist_mode=\"3d\",\n",
    "    misfit_engine=\"r2\",\n",
    "    window_seconds=5.0,\n",
    "    min_stations=5,\n",
    "    sam_class=VSAM,\n",
    "    sam_metric=\"mean\",\n",
    "    debug=False,\n",
    ").build()\n",
    "\n",
    "# ---------- 2) One-change variants ----------\n",
    "variants = cfg_variants_from(baseline_cfg)\n",
    "\n",
    "# ---------- 3) Locate each run’s CSV by tag ----------\n",
    "\n",
    "# ---------- 3.5) Auto-run switches ----------\n",
    "RUN_IF_MISSING_BASELINE = True\n",
    "RUN_IF_MISSING_VARIANTS = True\n",
    "\n",
    "\n",
    "# ---------- 4) Compare baseline vs each variant ----------\n",
    "base_tag = baseline_cfg.tag()\n",
    "\n",
    "for i, ev in zip(best_file_nums, best_event_files):\n",
    "    ev_key = Path(ev).stem\n",
    "    print(f\"\\n[{i}/{len(event_files)}] {ev_key}\")\n",
    "    event_dir = Path(OUTPUT_DIR) / ev_key\n",
    "\n",
    "    # Encode baseline in the filename to avoid clobbering across baselines\n",
    "    summary_csv = event_dir / f\"pairwise_{base_tag}_vs_variants.csv\"\n",
    "\n",
    "    # Baseline: auto-run only if missing\n",
    "    base_csv = ensure_csv_for(baseline_cfg, ev, is_baseline=True)\n",
    "    if base_csv is None:\n",
    "        print(\"  [skip] no baseline CSV; cannot compare.\")\n",
    "        continue\n",
    "\n",
    "    # Variants\n",
    "    for _, vcfg in variants.items():\n",
    "        alt_csv = ensure_csv_for(vcfg, ev, is_baseline=False)\n",
    "        try:\n",
    "            append_compare(summary_csv, base_csv, alt_csv, label=vcfg.tag())\n",
    "        except Exception as e:\n",
    "            print(f\"  [compare error] {vcfg.tag()}: {e}\")\n",
    "\n",
    "# ---------- 5) Roll-up ----------\n",
    "ROOT = OUTPUT_DIR\n",
    "allcmp = load_all_event_comparisons(ROOT)\n",
    "print(f\"\\nstacked rows: {len(allcmp)}, events: {allcmp['event_id'].nunique() if not allcmp.empty else 0}\")\n",
    "\n",
    "if not allcmp.empty:\n",
    "    scored  = add_composite_score(allcmp, w_sep=1.0, w_misfit=0.5, w_azgap=0.1)\n",
    "    summary = summarize_variants(scored)\n",
    "    winners, win_counts = per_event_winner(scored)\n",
    "    #print(summary)\n",
    "    display(summary)\n",
    "    display(win_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2000dcae",
   "metadata": {},
   "source": [
    "# Run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d57d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ✅ use the new module:\n",
    "from flovopy.asl.compare_runs import (\n",
    "    cfg_variants_from,\n",
    "    compare_runs,            # orchestrator\n",
    "    load_all_event_comparisons,  # optional if you want to re-load later\n",
    "    add_composite_score,         # optional\n",
    "    summarize_variants,          # optional\n",
    "    per_event_winner,            # optional\n",
    ")\n",
    "\n",
    "# --- Build baseline & variants ---\n",
    "landgridobj = Grid.load(GLOBAL_CACHE / \"land\" / \"Grid_9c2fd59b.pkl\")\n",
    "baseline_cfg = ASLConfig(\n",
    "    inventory=INV,\n",
    "    output_base=OUTPUT_DIR,\n",
    "    gridobj=gridobj,\n",
    "    global_cache=GLOBAL_CACHE,\n",
    "    station_correction_dataframe=station_corrections_df,\n",
    "    wave_kind=\"surface\",\n",
    "    speed=1.5,\n",
    "    Q=23,\n",
    "    peakf=2.0,\n",
    "    dist_mode=\"3d\",\n",
    "    misfit_engine=\"r2\",\n",
    "    window_seconds=5.0,\n",
    "    min_stations=5,\n",
    "    sam_class=VSAM,\n",
    "    sam_metric=\"mean\",\n",
    "    debug=False,\n",
    ").build()\n",
    "\n",
    "variants = cfg_variants_from(\n",
    "    baseline_cfg,\n",
    "    landgridobj=landgridobj,\n",
    "    annual_station_corrections_df=annual_station_corrections_df,\n",
    ")\n",
    "\n",
    "# --- Run comparisons (auto-run baseline & variants if missing) ---\n",
    "scored, summary, win_counts = compare_runs(\n",
    "    baseline_cfg,\n",
    "    events=best_event_files,            # iterable of mseed paths\n",
    "    variants=variants,\n",
    "    run_single_event=run_single_event,  # your existing function\n",
    "    refine_sector=False,\n",
    "    topo_kw=topo_kw,\n",
    "    run_if_missing_baseline=True,\n",
    "    run_if_missing_variants=True,       # flip to False if you want variants manual\n",
    "    w_sep=1.0, w_misfit=0.5, w_azgap=0.1,\n",
    ")\n",
    "\n",
    "# --- Show results ---\n",
    "if scored is not None:\n",
    "    display(summary)\n",
    "    display(win_counts)\n",
    "summary.to_csv(LOCALPROJECTDIR / \"pairwise_summary_surface.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd8621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flovopy.asl.compare_runs import compare_runs, build_intrinsic_table, add_baseline_free_scores, summarize_absolute_runs, per_event_winner_abs\n",
    "\n",
    "# 1) (optional) still run pairwise comparisons\n",
    "'''\n",
    "scored_pairwise, summary_pairwise, win_counts_pairwise = compare_runs(\n",
    "    baseline_cfg,\n",
    "    events=best_event_files,\n",
    "    variants=variants,\n",
    "    run_single_event=run_single_event,\n",
    "    refine_sector=False,\n",
    "    topo_kw=topo_kw,\n",
    "    run_if_missing_baseline=True,\n",
    "    run_if_missing_variants=True,\n",
    ")\n",
    "'''\n",
    "\n",
    "# 2) Build absolute (baseline-free) table and scores\n",
    "abs_tbl = build_intrinsic_table(\n",
    "    baseline_cfg,\n",
    "    events=best_event_files,\n",
    "    variants=variants,\n",
    "    run_single_event=run_single_event,\n",
    "    refine_sector=False,\n",
    "    topo_kw=topo_kw,\n",
    "    run_if_missing_baseline=False,   # set True if you want to autorun here too\n",
    "    run_if_missing_variants=False,\n",
    ")\n",
    "\n",
    "abs_scored = add_baseline_free_scores(abs_tbl)  # you can pass custom weights=...\n",
    "abs_summary = summarize_absolute_runs(abs_scored)\n",
    "winners_abs, win_counts_abs = per_event_winner_abs(abs_scored)\n",
    "\n",
    "abs_summary.to_csv(index=False)\n",
    "win_counts_abs.to_csv(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c5d40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flovopy.asl.compare_runs import crawl_intrinsic_runs\n",
    "abs_tbl = crawl_intrinsic_runs(OUTPUT_DIR)\n",
    "abs_scored  = add_baseline_free_scores(abs_tbl)\n",
    "abs_summary = summarize_absolute_runs(abs_scored)\n",
    "winners_abs, win_counts_abs = per_event_winner_abs(abs_scored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2d6690",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i from flovopy.asl.analyze_run_pairs import (...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198a0459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run ASL per event (cell 6) ---\n",
    "'''\n",
    "from typing import List, Dict, Any\n",
    "summaries: List[Dict[str, Any]] = []\n",
    "\n",
    "for i, ev in zip(best_file_nums, best_event_files):\n",
    "    print(f\"[{i}/{len(event_files)}] {ev}\")\n",
    "    result = run_single_event(\n",
    "        mseed_file=str(ev),\n",
    "        cfg=cfg,\n",
    "        refine_sector=REFINE_SECTOR,\n",
    "        station_gains_df=None,\n",
    "        topo_kw=topo_kw,\n",
    "        debug=True,\n",
    "    )\n",
    "    summaries.append(result)\n",
    "    break\n",
    "\n",
    "# Summarize\n",
    "df = pd.DataFrame(summaries)\n",
    "display(df)\n",
    "\n",
    "summary_csv = Path(OUTPUT_DIR) / f\"{cfg.tag()}__summary.csv\"\n",
    "df.to_csv(summary_csv, index=False)\n",
    "print(f\"Summary saved to: {summary_csv}\")\n",
    "\n",
    "if not df.empty:\n",
    "    n_ok = int((~df.get(\"error\").notna()).sum()) if \"error\" in df.columns else len(df)\n",
    "    print(f\"Success: {n_ok}/{len(df)}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39632ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_all_event_comparisons(root: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Crawl event folders under `root` and stack `pairwise_run_comparisons.csv`.\n",
    "    Returns a tidy DF with event_id inferred from folder name.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for csv in root.rglob(\"pairwise_run_comparisons.csv\"):\n",
    "        try:\n",
    "            df = pd.read_csv(csv)\n",
    "            df[\"event_id\"] = csv.parent.name            # the event folder name\n",
    "            rows.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"[skip] {csv}: {e}\")\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "    out = pd.concat(rows, ignore_index=True)\n",
    "    # normalize label text to a short key\n",
    "    out[\"variant\"] = out[\"label\"].astype(str)\n",
    "    # guard presence of expected columns\n",
    "    for c in [\"mean_sep_km\",\"delta_misfit_B_minus_A\",\"delta_azgap_B_minus_A\"]:\n",
    "        if c not in out.columns: out[c] = np.nan\n",
    "    return out\n",
    "\n",
    "def add_composite_score(df: pd.DataFrame,\n",
    "                        w_sep=1.0, w_misfit=0.5, w_azgap=0.1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Lower is better. Negative deltas are good if they reduce misfit/azgap.\n",
    "    \"\"\"\n",
    "    d = df.copy()\n",
    "    # z-score each metric for comparability (event-wise optional)\n",
    "    # here: global z-scores; switch to per-event z if events differ strongly in scale\n",
    "    for col in [\"mean_sep_km\",\"delta_misfit_B_minus_A\",\"delta_azgap_B_minus_A\"]:\n",
    "        x = d[col].to_numpy(dtype=float)\n",
    "        mu, sd = np.nanmean(x), np.nanstd(x) if np.nanstd(x)>0 else 1.0\n",
    "        d[col+\"_z\"] = (x - mu)/sd\n",
    "    d[\"score\"] = (\n",
    "        w_sep    * d[\"mean_sep_km_z\"] +\n",
    "        w_misfit * d[\"delta_misfit_B_minus_A_z\"] +\n",
    "        w_azgap  * d[\"delta_azgap_B_minus_A_z\"]\n",
    "    )\n",
    "    return d\n",
    "\n",
    "def summarize_variants(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    One line per variant: mean±SE of core metrics and composite score,\n",
    "    plus 'wins' (how often variant beats baseline the most for an event).\n",
    "    \"\"\"\n",
    "    g = df.groupby(\"variant\", dropna=False)\n",
    "    agg = g.agg(\n",
    "        n_events          = (\"event_id\", \"nunique\"),\n",
    "        n_rows            = (\"event_id\", \"size\"),\n",
    "        mean_sep_km_mean  = (\"mean_sep_km\", \"mean\"),\n",
    "        mean_sep_km_med   = (\"mean_sep_km\", \"median\"),\n",
    "        mean_sep_km_se    = (\"mean_sep_km\", lambda x: np.nanstd(x)/np.sqrt(max(1,(x.notna().sum())))),\n",
    "        dmisfit_mean      = (\"delta_misfit_B_minus_A\", \"mean\"),\n",
    "        dmisfit_med       = (\"delta_misfit_B_minus_A\", \"median\"),\n",
    "        dazgap_mean       = (\"delta_azgap_B_minus_A\", \"mean\"),\n",
    "        score_mean        = (\"score\", \"mean\"),\n",
    "        score_med         = (\"score\", \"median\"),\n",
    "    ).reset_index().sort_values(\"score_mean\")\n",
    "    return agg\n",
    "\n",
    "def per_event_winner(df_scored: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each event, pick the variant with the lowest composite score.\n",
    "    \"\"\"\n",
    "    # keep only the best per (event_id)\n",
    "    idx = df_scored.groupby(\"event_id\")[\"score\"].idxmin()\n",
    "    winners = df_scored.loc[idx, [\"event_id\",\"variant\",\"score\"]]\n",
    "    win_counts = winners.groupby(\"variant\").size().rename(\"wins\").reset_index()\n",
    "    return winners, win_counts.sort_values(\"wins\", ascending=False)\n",
    "\n",
    "# --- run it ---\n",
    "ROOT = OUTPUT_DIR  # your existing OUTDIR base\n",
    "allcmp = load_all_event_comparisons(ROOT)\n",
    "print(f\"stacked rows: {len(allcmp)}, events: {allcmp['event_id'].nunique()}\")\n",
    "\n",
    "scored = add_composite_score(allcmp, w_sep=1.0, w_misfit=0.5, w_azgap=0.1)\n",
    "summary = summarize_variants(scored)\n",
    "winners, win_counts = per_event_winner(scored)\n",
    "\n",
    "# quick looks\n",
    "display(summary.head(10))\n",
    "display(win_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a47df7",
   "metadata": {},
   "source": [
    "# Run all events efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552236da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(INPUT_DIR)\n",
    "print(cfg)\n",
    "print(topo_kw)\n",
    "print(REFINE_SECTOR)\n",
    "'''\n",
    "run_all_events(\n",
    "    input_dir=INPUT_DIR,\n",
    "    station_gains_df = None,\n",
    "    cfg=cfg,\n",
    "    refine_sector=REFINE_SECTOR,\n",
    "    topo_kw=topo_kw,\n",
    "    debug=True,\n",
    "    max_events=999999,\n",
    "    use_multiprocessing=True,\n",
    "    workers=4,\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613e3b64",
   "metadata": {},
   "source": [
    "# Run Monte Carlo sweep of parameters for 1 event\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13b73fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from flovopy.asl.wrappers2 import run_event_monte_carlo\n",
    "from flovopy.processing.sam import VSAM, DSAM\n",
    "'''\n",
    "# Simple 6-draw sweep (replace with your own priors/sequences)\n",
    "configs = ASLConfig.generate_config_list(\n",
    "    inventory=None,\n",
    "    output_base=None,\n",
    "    gridobj=None,\n",
    "    global_cache=None,      \n",
    "    wave_kinds=(\"surface\",\"body\"),\n",
    "    station_corr_tables=(station_corrections_df), #annual_station_corrections_df),\n",
    "    speeds=(1.0, 3.0),\n",
    "    Qs=(23, 1000),\n",
    "    dist_modes=(\"3d\",), # 2d needs a different grid and different distance and amplitude corrections\n",
    "    misfit_engines=(\"l2\",\"r2\", \"lin\"),\n",
    "    peakfs=(2.0, 8.0),\n",
    "    window_seconds = 5.0, # change to be a tuple 10.0) not implemented yet\n",
    "    min_stations = 5,\n",
    "    sam_class = (VSAM), #, DSAM), # not implemented yet\n",
    "    sam_metric = (\"mean\"),# \"median\", \"rms\", \"VT\", \"LP\"), # this doesn't seem to be implemented yet\n",
    "    # context can be set later; set here if you like:\n",
    "    debug=False,\n",
    ")\n",
    "\n",
    "configs = ASLConfig.generate_config_list(\n",
    "    inventory=None,\n",
    "    output_base=None,\n",
    "    gridobj=None,\n",
    "    global_cache=None,      \n",
    "    wave_kinds=(\"surface\",),\n",
    "    station_corr_tables=(station_corrections_df), #annual_station_corrections_df),\n",
    "    speeds=(1.0, 3.0),\n",
    "    Qs=(23, 1000),\n",
    "    dist_modes=(\"3d\",), # 2d needs a different grid and different distance and amplitude corrections\n",
    "    misfit_engines=(\"l2\"),\n",
    "    peakfs=(8.0),\n",
    "    window_seconds = 5.0, # change to be a tuple 10.0) not implemented yet\n",
    "    min_stations = 5,\n",
    "    sam_class = (VSAM), #, DSAM), # not implemented yet\n",
    "    sam_metric = (\"mean\"),# \"median\", \"rms\", \"VT\", \"LP\"), # this doesn't seem to be implemented yet\n",
    "    # context can be set later; set here if you like:\n",
    "    debug=False,\n",
    ")\n",
    "\n",
    "\n",
    "configs = ASLConfig.generate_config_list(    \n",
    "    inventory=INV,\n",
    "    output_base=str(OUTPUT_DIR),\n",
    "    gridobj=gridobj,\n",
    "    global_cache=GLOBAL_CACHE,\n",
    ") \n",
    "\n",
    "print(len(configs))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21faaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Shared run context\n",
    "mseed_file   = event_files[116]\n",
    "'''\n",
    "results = run_event_monte_carlo(\n",
    "    mseed_file=mseed_file,\n",
    "    configs=configs,\n",
    "    inventory=INV,\n",
    "    output_base=str(OUTPUT_DIR),\n",
    "    gridobj=gridobj,\n",
    "    topo_kw=topo_kw,\n",
    "    station_gains_df=None,\n",
    "    parallel=False,\n",
    "    max_workers=1,\n",
    "    global_cache=GLOBAL_CACHE,\n",
    "    debug=True,\n",
    ")\n",
    "\n",
    "# Inspect or summarize results as needed\n",
    "n_ok = sum(1 for r in results if \"error\" not in r)\n",
    "print(f\"[MC] Completed {n_ok}/{len(results)} runs OK\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flovopy_plus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
